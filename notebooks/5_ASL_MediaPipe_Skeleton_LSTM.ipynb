{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478dfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b9d4c6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 2: Import Libraries\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import sklearn components with fallback\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "    print(\"✓ scikit-learn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠ scikit-learn import issue: {e}\")\n",
    "    print(\"  Installing latest scikit-learn...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'scikit-learn'])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "    print(\"✓ scikit-learn re-imported successfully\")\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"MediaPipe Version: {mp.__version__}\")\n",
    "print(f\"OpenCV Version: {cv2.__version__}\")\n",
    "\n",
    "# GPU Check\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"✓ GPU Available: {len(gpus)} device(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu}\")\n",
    "else:\n",
    "    print(\"⚠ No GPU detected - will use CPU (slower)\")\n",
    "\n",
    "print(\"\\nAll imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 3: Configuration\n",
    "# ============================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'NUM_CLASSES': 28,  # A-Z (26) + space + nothing\n",
    "    'SEQUENCE_LENGTH': 30,  # frames per gesture\n",
    "    'NUM_KEYPOINTS': 21,  # MediaPipe hand keypoints\n",
    "    'KEYPOINT_DIMS': 2,  # x, y coordinates (no z for simplicity)\n",
    "    \n",
    "    # Model\n",
    "    'LSTM_UNITS_1': 128,\n",
    "    'LSTM_UNITS_2': 64,\n",
    "    'DENSE_UNITS': 32,\n",
    "    'DROPOUT': 0.3,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 50,\n",
    "    \n",
    "    # Training\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'VALIDATION_SPLIT': 0.2,\n",
    "    'TEST_SPLIT': 0.1,\n",
    "    \n",
    "    # Paths\n",
    "    'OUTPUT_DIR': 'output',\n",
    "    'MODEL_PATH': 'output/asl_lstm_model.h5',\n",
    "    'SAVEDMODEL_PATH': 'models/asl_lstm_savedmodel',\n",
    "    'TFJS_PATH': 'output/tfjs_asl_lstm',\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['OUTPUT_DIR'], exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Class labels\n",
    "CLASSES = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ') + ['space', 'nothing']\n",
    "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "IDX_TO_CLASS = {i: c for i, c in enumerate(CLASSES)}\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if not key.endswith('_PATH'):\n",
    "        print(f\"  {key:20s}: {value}\")\n",
    "\n",
    "print(f\"\\nClasses ({len(CLASSES)}): {', '.join(CLASSES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659e0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4 (New): Keypoint Extraction Setup\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm # For progress bar\n",
    "\n",
    "# --- Configuration for Keypoint Extraction ---\n",
    "# Update DATA_DIR to point to the correct, *full* ASL Alphabet directory (A/B/.../Z/space/)\n",
    "DATA_DIR_IMAGES = Path(\"../datasets/ASL Dataset/asl_alphabet_train\") \n",
    "OUTPUT_CSV_PATH = Path(\"../datasets/ASL_Keypoints.csv\")\n",
    "MAX_NUM_HANDS = 1 # We are classifying single letters\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True, \n",
    "    max_num_hands=MAX_NUM_HANDS, \n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "def extract_and_save_keypoints(image_dir, output_csv):\n",
    "    print(f\"Starting keypoint extraction from: {image_dir}\")\n",
    "    all_landmarks = []\n",
    "    \n",
    "    # Header for CSV: 'class', 'x0', 'y0', 'z0', 'x1', 'y1', 'z1', ..., 'x20', 'y20', 'z20'\n",
    "    header = ['class']\n",
    "    for i in range(21): # 21 landmarks per hand\n",
    "        header.extend([f'x{i}', f'y{i}', f'z{i}'])\n",
    "\n",
    "    with open(output_csv, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Iterate through subdirectories (classes)\n",
    "        class_dirs = [d for d in image_dir.iterdir() if d.is_dir()]\n",
    "        for class_dir in tqdm(class_dirs, desc=\"Processing Classes\"):\n",
    "            class_name = class_dir.name\n",
    "            \n",
    "            # Iterate through images in class directory\n",
    "            for image_path in class_dir.glob('*.jpg'):\n",
    "                try:\n",
    "                    image = cv2.imread(str(image_path))\n",
    "                    if image is None: continue\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    results = hands.process(image)\n",
    "                    \n",
    "                    if results.multi_hand_landmarks:\n",
    "                        # Only take the first detected hand for simplicity\n",
    "                        landmarks = results.multi_hand_landmarks[0].landmark\n",
    "                        \n",
    "                        # Flatten the normalized coordinates (x, y, z) into a single row vector\n",
    "                        row = [class_name]\n",
    "                        for landmark in landmarks:\n",
    "                            # Normalize coordinates by hand location (e.g., relative to wrist)\n",
    "                            # Simple normalization: raw (x, y, z) here. Better: relative to wrist (lm[0])\n",
    "                            row.extend([landmark.x, landmark.y, landmark.z])\n",
    "                        \n",
    "                        writer.writerow(row)\n",
    "                except Exception as e:\n",
    "                    # print(f\"Error processing {image_path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "    print(f\"\\n✓ Keypoint data saved to: {output_csv}\")\n",
    "    print(\"  Run this cell only once to generate the CSV dataset!\")\n",
    "\n",
    "# UNCOMMENT THE LINE BELOW TO RUN KEYPOINT EXTRACTION!\n",
    "# extract_and_save_keypoints(DATA_DIR_IMAGES, OUTPUT_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3bda2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Data generation functions ready\n",
      "  - Can load video files and extract keypoints\n",
      "  - Can generate synthetic data for testing\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 5: Data Generation Functions\n",
    "# ============================================\n",
    "\n",
    "def extract_gesture_sequence(video_path, gesture_class, sequence_length=30):\n",
    "    \"\"\"\n",
    "    Extract hand keypoint sequence from a video file.\n",
    "    \n",
    "    Args:\n",
    "        video_path: path to video file\n",
    "        gesture_class: class label (e.g., 'A', 'B', 'space')\n",
    "        sequence_length: number of frames to extract\n",
    "    \n",
    "    Returns:\n",
    "        sequence: array of shape (sequence_length, 21, 2) or None\n",
    "        gesture_class: class label\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None, gesture_class\n",
    "    \n",
    "    frames_keypoints = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    while len(frames_keypoints) < sequence_length:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Extract hand keypoints\n",
    "        keypoints = extractor.extract(frame)\n",
    "        \n",
    "        if keypoints is not None:\n",
    "            frames_keypoints.append(keypoints)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Pad or trim to sequence_length\n",
    "    if len(frames_keypoints) == 0:\n",
    "        return None, gesture_class\n",
    "    \n",
    "    if len(frames_keypoints) < sequence_length:\n",
    "        # Pad with last frame\n",
    "        last_frame = frames_keypoints[-1]\n",
    "        while len(frames_keypoints) < sequence_length:\n",
    "            frames_keypoints.append(last_frame)\n",
    "    else:\n",
    "        # Subsample if too long\n",
    "        indices = np.linspace(0, len(frames_keypoints)-1, sequence_length, dtype=int)\n",
    "        frames_keypoints = [frames_keypoints[i] for i in indices]\n",
    "    \n",
    "    sequence = np.array(frames_keypoints)  # shape: (sequence_length, 21, 2)\n",
    "    return sequence, gesture_class\n",
    "\n",
    "def generate_synthetic_data(num_samples_per_class=50, sequence_length=30):\n",
    "    \"\"\"\n",
    "    Generate synthetic hand gesture data for testing and demonstration.\n",
    "    \n",
    "    In production, replace this with actual gesture videos from:\n",
    "    - ASL Alphabet Dataset\n",
    "    - ASL-LEX Dataset\n",
    "    - Custom recordings\n",
    "    \n",
    "    Returns:\n",
    "        X: sequences of shape (total_samples, sequence_length, 21, 2)\n",
    "        y: class labels\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    print(f\"Generating {num_samples_per_class} synthetic samples per class...\")\n",
    "    \n",
    "    for class_idx, class_name in enumerate(CLASSES):\n",
    "        for sample_idx in range(num_samples_per_class):\n",
    "            # Generate random walk for hand keypoints\n",
    "            sequence = np.zeros((sequence_length, 21, 2))\n",
    "            \n",
    "            # Create realistic motion patterns\n",
    "            base_x = np.random.uniform(0.3, 0.7)\n",
    "            base_y = np.random.uniform(0.3, 0.7)\n",
    "            \n",
    "            for frame_idx in range(sequence_length):\n",
    "                # Add motion\n",
    "                t = frame_idx / sequence_length\n",
    "                \n",
    "                # Different motion patterns per class (for variety)\n",
    "                motion_x = np.sin(class_idx * t * np.pi) * 0.1\n",
    "                motion_y = np.cos(class_idx * t * np.pi * 2) * 0.1\n",
    "                \n",
    "                # Random noise for each keypoint\n",
    "                for kp_idx in range(21):\n",
    "                    noise = np.random.normal(0, 0.02, 2)\n",
    "                    x = base_x + motion_x + noise[0]\n",
    "                    y = base_y + motion_y + noise[1]\n",
    "                    \n",
    "                    # Clip to [0, 1]\n",
    "                    sequence[frame_idx, kp_idx] = [\n",
    "                        np.clip(x, 0, 1),\n",
    "                        np.clip(y, 0, 1)\n",
    "                    ]\n",
    "            \n",
    "            X.append(sequence)\n",
    "            y.append(CLASS_TO_IDX[class_name])\n",
    "        \n",
    "        if (class_idx + 1) % 7 == 0:\n",
    "            print(f\"  Generated {(class_idx + 1) * num_samples_per_class} samples...\")\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "print(\"\\n✓ Data generation functions ready\")\n",
    "print(f\"  - Can load video files and extract keypoints\")\n",
    "print(f\"  - Can generate synthetic data for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f0d26ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing dataset...\n",
      "============================================================\n",
      "Generating 20 synthetic samples per class...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m NUM_SAMPLES_PER_CLASS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# Increase to 100+ for better model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# This line should now work after fixing the list reassignment issue inside the function.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_synthetic_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples_per_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_SAMPLES_PER_CLASS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSEQUENCE_LENGTH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Use np.unique to get the unique class counts robustly\u001b[39;00m\n\u001b[0;32m     20\u001b[0m unique_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "Cell \u001b[1;32mIn[8], line 105\u001b[0m, in \u001b[0;36mgenerate_synthetic_data\u001b[1;34m(num_samples_per_class, sequence_length)\u001b[0m\n\u001b[0;32m     99\u001b[0m             sequence[frame_idx, kp_idx] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    100\u001b[0m                 np\u001b[38;5;241m.\u001b[39mclip(x, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    101\u001b[0m                 np\u001b[38;5;241m.\u001b[39mclip(y, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    102\u001b[0m             ]\n\u001b[0;32m    104\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(sequence)\n\u001b[1;32m--> 105\u001b[0m     \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(CLASS_TO_IDX[class_name])\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (class_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(class_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mnum_samples_per_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 6: Generate or Load Data\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nPreparing dataset...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For production, replace with actual videos\n",
    "# X, y = load_real_videos_from_dataset()\n",
    "\n",
    "# For demonstration, use synthetic data\n",
    "NUM_SAMPLES_PER_CLASS = 20  # Increase to 100+ for better model\n",
    "\n",
    "X, y = generate_synthetic_data(\n",
    "    num_samples_per_class=NUM_SAMPLES_PER_CLASS,\n",
    "    sequence_length=CONFIG['SEQUENCE_LENGTH']\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Shape:\")\n",
    "print(f\"  X: {X.shape} (samples, frames, keypoints, coords)\")\n",
    "print(f\"  y: {y.shape} (class indices)\")\n",
    "print(f\"  Total samples: {len(X)}\")\n",
    "print(f\"  Total classes: {len(np.unique(y))}\")\n",
    "print(f\"  Samples per class: {NUM_SAMPLES_PER_CLASS}\")\n",
    "\n",
    "# Normalize keypoint coordinates to [-1, 1]\n",
    "print(f\"\\nNormalizing data...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X.reshape(len(X), -1))\n",
    "X_scaled = X_scaled.reshape(X.shape)\n",
    "\n",
    "print(f\"✓ Data normalized\")\n",
    "print(f\"  - Mean: {X_scaled.mean():.4f}\")\n",
    "print(f\"  - Std: {X_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e2a305",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 45) (2035478964.py, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 45\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"  - Std (should be near 1): {X_scaled.std():.4f}\")\"\u001b[0m\n\u001b[1;37m                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 45)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7: Split Data\n",
    "# ============================================\n",
    "\n",
    "# First split: train + val vs test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=CONFIG['TEST_SPLIT'],\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: train vs validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=CONFIG['VALIDATION_SPLIT'],\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"\\nData Split:\")\n",
    "print(f\"  Training   : {X_train.shape[0]} samples\")\n",
    "print(f\"  Validation : {X_val.shape[0]} samples\")\n",
    "print(f\"  Test       : {X_test.shape[0]} samples\")\n",
    "print(f\"  Total      : {len(X)} samples\")\n",
    "\n",
    "# Class distribution\n",
    "print(f\"\\nClass Distribution (Training):\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for class_idx, count in zip(unique[:5], counts[:5]):\n",
    "    print(f\"  {CLASSES[class_idx]:10s}: {count:3d} samples\")\n",
    "print(f\"  ... ({len(unique)} total classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859866f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 8: Build LSTM Model\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nBuilding LSTM model for ASL recognition...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def build_lstm_model(config):\n",
    "    \"\"\"\n",
    "    Build LSTM model for hand gesture recognition.\n",
    "    \n",
    "    Input shape: (batch_size, sequence_length, num_keypoints, keypoint_dims)\n",
    "    Output shape: (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    input_shape = (\n",
    "        config['SEQUENCE_LENGTH'],\n",
    "        config['NUM_KEYPOINTS'],\n",
    "        config['KEYPOINT_DIMS']\n",
    "    )\n",
    "    \n",
    "    # Reshape input to 3D for LSTM\n",
    "    # (sequence_length, num_keypoints*keypoint_dims)\n",
    "    inputs = keras.Input(shape=input_shape, name='hand_sequence')\n",
    "    \n",
    "    # Flatten keypoints for LSTM\n",
    "    x = layers.Reshape((\n",
    "        config['SEQUENCE_LENGTH'],\n",
    "        config['NUM_KEYPOINTS'] * config['KEYPOINT_DIMS']\n",
    "    ))(inputs)\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = layers.LSTM(\n",
    "        config['LSTM_UNITS_1'],\n",
    "        return_sequences=True,\n",
    "        name='lstm_1'\n",
    "    )(x)\n",
    "    x = layers.Dropout(config['DROPOUT'])(x)\n",
    "    \n",
    "    x = layers.LSTM(\n",
    "        config['LSTM_UNITS_2'],\n",
    "        return_sequences=False,\n",
    "        name='lstm_2'\n",
    "    )(x)\n",
    "    x = layers.Dropout(config['DROPOUT'])(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(config['DENSE_UNITS'], activation='relu', name='dense')(x)\n",
    "    x = layers.Dropout(config['DROPOUT'])(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Dense(config['NUM_CLASSES'], activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='ASL_LSTM')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_lstm_model(CONFIG)\n",
    "\n",
    "# Compile\n",
    "optimizer = keras.optimizers.Adam(learning_rate=CONFIG['LEARNING_RATE'])\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Summary\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\n✓ Model built successfully\")\n",
    "print(f\"  - Input shape: {model.input_shape}\")\n",
    "print(f\"  - Output shape: {model.output_shape}\")\n",
    "print(f\"  - Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bcef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 9: Train Model\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    epochs=CONFIG['EPOCHS'],\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 10: Evaluate Model\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nEvaluating model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Metrics\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, y_pred,\n",
    "    target_names=[CLASSES[i] for i in range(CONFIG['NUM_CLASSES'])],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix Shape: {conf_matrix.shape}\")\n",
    "print(f\"Correct predictions (diagonal): {np.trace(conf_matrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 11: Visualize Results\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training history\n",
    "axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0, 0].set_title('Model Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0, 1].set_title('Model Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(\n",
    "    conf_matrix[:10, :10],  # First 10 classes for visibility\n",
    "    annot=False,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    ax=axes[1, 0]\n",
    ")\n",
    "axes[1, 0].set_title('Confusion Matrix (First 10 Classes)')\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "\n",
    "# Class-wise accuracy\n",
    "class_accuracies = np.diag(conf_matrix) / conf_matrix.sum(axis=1)\n",
    "axes[1, 1].barh(CLASSES, class_accuracies)\n",
    "axes[1, 1].set_title('Per-Class Accuracy')\n",
    "axes[1, 1].set_xlabel('Accuracy')\n",
    "axes[1, 1].set_xlim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['OUTPUT_DIR']}/training_results.png\", dpi=100, bbox_inches='tight')\n",
    "print(\"\\n✓ Saved training_results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 12: Save Model\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save as HDF5\n",
    "model.save(CONFIG['MODEL_PATH'])\n",
    "print(f\"✓ Saved: {CONFIG['MODEL_PATH']}\")\n",
    "\n",
    "# Save as SavedModel\n",
    "tf.saved_model.save(model, CONFIG['SAVEDMODEL_PATH'])\n",
    "print(f\"✓ Saved: {CONFIG['SAVEDMODEL_PATH']}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_type': 'LSTM Hand Gesture Recognition',\n",
    "    'num_classes': CONFIG['NUM_CLASSES'],\n",
    "    'sequence_length': CONFIG['SEQUENCE_LENGTH'],\n",
    "    'num_keypoints': CONFIG['NUM_KEYPOINTS'],\n",
    "    'classes': CLASSES,\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'input_shape': [None, CONFIG['SEQUENCE_LENGTH'], CONFIG['NUM_KEYPOINTS'], CONFIG['KEYPOINT_DIMS']],\n",
    "    'output_shape': [None, CONFIG['NUM_CLASSES']],\n",
    "    'architecture': 'LSTM(128) -> LSTM(64) -> Dense(32)',\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'test_samples': len(X_test)\n",
    "}\n",
    "\n",
    "with open(f\"{CONFIG['OUTPUT_DIR']}/model_metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved: model_metadata.json\")\n",
    "print(f\"\\nModel Info:\")\n",
    "for key, value in metadata.items():\n",
    "    if key != 'classes':\n",
    "        print(f\"  {key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923af801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 13: Convert to TFJS (Optional)\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "\n",
    "print(\"\\nConverting to TensorFlow.js format...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Note: This requires tensorflowjs package\n",
    "# pip install tensorflowjs\n",
    "\n",
    "try:\n",
    "    import tensorflowjs as tfjs\n",
    "    \n",
    "    # Create TFJS output directory\n",
    "    os.makedirs(CONFIG['TFJS_PATH'], exist_ok=True)\n",
    "    \n",
    "    # Convert\n",
    "    tfjs.converters.save_keras_model(model, CONFIG['TFJS_PATH'])\n",
    "    \n",
    "    print(f\"✓ Converted to TFJS: {CONFIG['TFJS_PATH']}\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    for file in os.listdir(CONFIG['TFJS_PATH']):\n",
    "        file_path = os.path.join(CONFIG['TFJS_PATH'], file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"  - {file:40s} ({size:,} bytes)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⚠ tensorflowjs not installed\")\n",
    "    print(\"  To convert, run: pip install tensorflowjs\")\n",
    "    print(\"  Then: tensorflowjs_converter --input_format=keras_saved_model --output_format=tfjs_graph_model\")\n",
    "    print(f\"       {CONFIG['SAVEDMODEL_PATH']} {CONFIG['TFJS_PATH']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fd501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 14: Test Real-Time Inference\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nTesting real-time inference...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def predict_gesture(sequence, model, classes):\n",
    "    \"\"\"\n",
    "    Predict gesture from a sequence of hand keypoints.\n",
    "    \n",
    "    Args:\n",
    "        sequence: array of shape (sequence_length, num_keypoints, 2)\n",
    "        model: trained keras model\n",
    "        classes: list of class names\n",
    "    \n",
    "    Returns:\n",
    "        prediction: dict with class, confidence, top-3\n",
    "    \"\"\"\n",
    "    # Add batch dimension\n",
    "    input_seq = np.expand_dims(sequence, axis=0)\n",
    "    \n",
    "    # Predict\n",
    "    probabilities = model.predict(input_seq, verbose=0)[0]\n",
    "    \n",
    "    # Get top-3\n",
    "    top_3_idx = np.argsort(probabilities)[-3:][::-1]\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': classes[np.argmax(probabilities)],\n",
    "        'confidence': float(np.max(probabilities)),\n",
    "        'top_3': [\n",
    "            {'class': classes[idx], 'probability': float(probabilities[idx])}\n",
    "            for idx in top_3_idx\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Test on a few samples\n",
    "print(f\"\\nTest Predictions (First 5 test samples):\")\n",
    "for i in range(min(5, len(X_test))):\n",
    "    test_seq = X_test[i]\n",
    "    true_class = CLASSES[y_test[i]]\n",
    "    \n",
    "    result = predict_gesture(test_seq, model, CLASSES)\n",
    "    \n",
    "    match = \"✓\" if result['predicted_class'] == true_class else \"✗\"\n",
    "    print(f\"\\n  Sample {i+1}: {match}\")\n",
    "print(f\"    True Class    : {true_class}\")\n",
    "    print(f\"    Predicted    : {result['predicted_class']}\")\n",
    "    print(f\"    Confidence   : {result['confidence']*100:.2f}%\")\n",
    "    print(f\"    Top-3        : \", end=\"\")\n",
    "    for pred in result['top_3']:\n",
    "        print(f\"{pred['class']} ({pred['probability']*100:.1f}%) \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d58c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 15: Integration with Web App\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nWeb App Integration Guide\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "## How to Use in Web Application\n",
    "\n",
    "### 1. Copy TFJS Model Files\n",
    "Copy the generated TFJS files to your web app:\n",
    "```\n",
    "cp -r output/tfjs_asl_lstm/* public/models/\n",
    "```\n",
    "\n",
    "### 2. Update Web Component\n",
    "\n",
    "In your React/Vue/etc component:\n",
    "\n",
    "```javascript\n",
    "// Load model\n",
    "const model = await tf.loadLayersModel('/models/model.json');\n",
    "\n",
    "// Load hand landmarks (use MediaPipe)\n",
    "const landmarks = await extractHandLandmarks(videoFrame);\n",
    "\n",
    "// Create sequence (30 frames x 21 keypoints x 2 coords)\n",
    "const sequence = tf.tensor3d(your_landmarks_sequence);\n",
    "\n",
    "// Predict\n",
    "const output = model.predict(sequence);\n",
    "const probabilities = await output.data();\n",
    "const classIndex = tf.argMax(probabilities).dataSync()[0];\n",
    "const className = CLASSES[classIndex];\n",
    "```\n",
    "\n",
    "### 3. Benefits of LSTM Model\n",
    "✓ Captures temporal hand movements\n",
    "✓ Robust to lighting conditions\n",
    "✓ Works with different hand sizes\n",
    "✓ Small model size (fast inference)\n",
    "✓ Privacy-friendly (keypoints only)\n",
    "\n",
    "### 4. Production Improvements\n",
    "- Use real gesture videos (not synthetic data)\n",
    "- Collect more samples per class (100+)\n",
    "- Fine-tune with user-specific data\n",
    "- Add data augmentation\n",
    "- Implement confidence thresholds\n",
    "- Add gesture sequences (multi-letter words)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
