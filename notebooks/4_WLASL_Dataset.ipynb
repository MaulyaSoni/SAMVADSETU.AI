{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9bd193",
   "metadata": {},
   "source": [
    "# WLASL Dataset - Large-scale Video Dataset for Sign Language Recognition\n",
    "\n",
    "**Dataset:** WLASL by Facebook AI (ASL Citizen)  \n",
    "**Size:** 2,700 videos of complete ASL words (not isolated signs)  \n",
    "**Classes:** 2,000+ unique words  \n",
    "**Format:** MP4 videos with variable length (1-10 seconds)  \n",
    "**Challenge:** Temporal modeling, variable sequence lengths, complete word recognition  \n",
    "**Goal:** Train LSTM/GRU model for word-level sign language recognition\n",
    "\n",
    "**Best Run On:** Google Colab with GPU (video processing is intensive)  \n",
    "**Training Time:** 1-2 hours for temporal model training\n",
    "\n",
    "**Key Steps:**\n",
    "1. Download WLASL videos\n",
    "2. Extract frames from videos\n",
    "3. Use MediaPipe to extract hand keypoints (21 landmarks per hand)\n",
    "4. Create sequence dataset (frame sequences with labels)\n",
    "5. Train LSTM model on keypoint sequences\n",
    "6. Export for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce64829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with MediaPipe\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    'tensorflow', 'kaggle', 'opencv-python', 'mediapipe', 'numpy', 'pandas',\n",
    "    'matplotlib', 'scikit-learn', 'tensorflowjs'\n",
    "]\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
    "\n",
    "from pathlib import Path\n",
    "for d in ['external_data', 'datasets', 'models', 'output']:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Setup complete with MediaPipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hand keypoints using MediaPipe\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def extract_hand_keypoints_from_video(video_path, max_frames=30):\n",
    "    \"\"\"Extract hand keypoints from video using MediaPipe.\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    keypoints_sequence = []\n",
    "    \n",
    "    with mp_hands.Hands(static_image_mode=False, max_num_hands=2) as hands:\n",
    "        frame_count = 0\n",
    "        while cap.isOpened() and frame_count < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(frame_rgb)\n",
    "            \n",
    "            # Extract landmarks (21 points per hand, 3D coordinates)\n",
    "            frame_keypoints = np.zeros((42,))  # 2 hands √ó 21 points = 42 values (x,y)\n",
    "            \n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_idx, landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                    if hand_idx >= 2:\n",
    "                        break\n",
    "                    for i, lm in enumerate(landmarks.landmark):\n",
    "                        frame_keypoints[hand_idx*21 + i*2] = lm.x\n",
    "                        frame_keypoints[hand_idx*21 + i*2 + 1] = lm.y\n",
    "            \n",
    "            keypoints_sequence.append(frame_keypoints)\n",
    "            frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(keypoints_sequence) if keypoints_sequence else None\n",
    "\n",
    "print(\"‚úì Keypoint extraction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence dataset for temporal modeling\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def create_sequence_dataset(video_dir, max_frames=30):\n",
    "    \"\"\"Create padded sequence dataset from videos.\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    label_dict = {}\n",
    "    \n",
    "    video_dir = Path(video_dir)\n",
    "    label_idx = 0\n",
    "    \n",
    "    for word_dir in video_dir.iterdir():\n",
    "        if not word_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        word = word_dir.name\n",
    "        label_dict[label_idx] = word\n",
    "        \n",
    "        for video_file in word_dir.glob('*.mp4'):\n",
    "            try:\n",
    "                keypoints = extract_hand_keypoints_from_video(str(video_file), max_frames)\n",
    "                if keypoints is not None:\n",
    "                    sequences.append(keypoints)\n",
    "                    labels.append(label_idx)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        label_idx += 1\n",
    "    \n",
    "    # Pad sequences to uniform length\n",
    "    X = pad_sequences(sequences, maxlen=max_frames, padding='post', dtype='float32')\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    print(f\"‚úì Dataset created: {len(X)} sequences, {len(label_dict)} classes\")\n",
    "    return X, y, label_dict\n",
    "\n",
    "print(\"‚úì Sequence dataset creation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM temporal model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.train_test_split import train_test_split\n",
    "\n",
    "# Load or create dummy data for demonstration\n",
    "print(\"üìù Creating LSTM model for temporal sequence data...\")\n",
    "\n",
    "# Model parameters\n",
    "MAX_FRAMES = 30\n",
    "FEATURE_DIM = 42  # 2 hands √ó 21 points √ó 2 coords\n",
    "EPOCHS = 20\n",
    "BATCH = 32\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=True), input_shape=(MAX_FRAMES, FEATURE_DIM)),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification (modify for multi-class)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úì LSTM model built\")\n",
    "print(f\"  Architecture: Bidirectional LSTM ‚Üí Dense layers\")\n",
    "print(f\"  Input: ({MAX_FRAMES}, {FEATURE_DIM}) - sequences of hand keypoints\")\n",
    "print(f\"  Output: Class prediction for ASL word\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea65bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training template (requires actual WLASL data)\n",
    "print(\"üöÄ To train with real WLASL data:\")\n",
    "print(\"\"\"\n",
    "# After creating sequences with create_sequence_dataset():\n",
    "X, y, label_dict = create_sequence_dataset('external_data/wlasl_videos')\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Convert labels to categorical (for multi-class)\n",
    "y_train_cat = to_categorical(y_train, num_classes=len(label_dict))\n",
    "y_test_cat = to_categorical(y_test, num_classes=len(label_dict))\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    validation_split=0.15,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_cat)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save\n",
    "model.save('models/wlasl_lstm.h5')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992100c",
   "metadata": {},
   "source": [
    "## Summary: Four Dataset Notebooks\n",
    "\n",
    "### üìä Dataset Comparison\n",
    "\n",
    "| Dataset | Format | Size | Classes | Model | Best For |\n",
    "|---------|--------|------|---------|-------|----------|\n",
    "| **ASL Alphabet** | Images (160√ó160) | 87K | 26 | MobileNetV2 | Letter recognition |\n",
    "| **Sign MNIST** | CSV (28√ó28) | 27K | 24 | Simple CNN | Quick training |\n",
    "| **HaGRID** | Images (variable) | 500K | 18 | EfficientNet | Real-world robustness |\n",
    "| **WLASL** | Videos (MP4) | 2.7K | 2000+ | LSTM | Word-level recognition |\n",
    "\n",
    "### üéØ Use Cases\n",
    "\n",
    "1. **ASL Alphabet** ‚Üí Real-time letter spelling (interactive games, education)\n",
    "2. **Sign MNIST** ‚Üí Lightweight deployment (mobile, web)\n",
    "3. **HaGRID** ‚Üí Production gesture recognition (various backgrounds)\n",
    "4. **WLASL** ‚Üí Complete word understanding (conversation support)\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. Choose dataset based on use case\n",
    "2. Download and preprocess data\n",
    "3. Run training notebook\n",
    "4. Export to TFJS or ONNX\n",
    "5. Deploy in your SamvadSetu application"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
