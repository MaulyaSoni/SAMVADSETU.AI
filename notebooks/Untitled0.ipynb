{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uRlsDh3MRi_0",
    "outputId": "76aa96af-6e53-489d-89d2-c2f50e0a25a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision: <DTypePolicy \"mixed_float16\">\n"
     ]
    }
   ],
   "source": [
    "# Mixed precision for speed + lower VRAM\n",
    "import tensorflow as tf\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "print(\"Mixed precision:\", tf.keras.mixed_precision.global_policy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bU6h8E-FRsQV",
    "outputId": "28ce2177-0ba6-4de4-cf02-5dfae521280f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe==0.10.21\n",
      "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (25.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (25.9.23)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (0.7.2)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (0.7.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (3.10.0)\n",
      "Collecting numpy<2 (from mediapipe==0.10.21)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (4.12.0.88)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.21)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.21)\n",
      "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21) (0.2.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.21) (2.0.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21) (0.5.4)\n",
      "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax (from mediapipe==0.10.21)\n",
      "  Downloading jax-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from mediapipe==0.10.21)\n",
      "  Downloading jaxlib-0.8.2-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting jax (from mediapipe==0.10.21)\n",
      "  Downloading jax-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from mediapipe==0.10.21)\n",
      "  Downloading jaxlib-0.8.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting jax (from mediapipe==0.10.21)\n",
      "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from mediapipe==0.10.21)\n",
      "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting jax (from mediapipe==0.10.21)\n",
      "  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from mediapipe==0.10.21)\n",
      "  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21) (1.16.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21) (2.9.0.post0)\n",
      "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-contrib-python (from mediapipe==0.10.21)\n",
      "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.21) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.21) (1.17.0)\n",
      "Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
      "Downloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, jaxlib, jax, mediapipe\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: opencv-contrib-python\n",
      "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
      "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
      "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
      "  Attempting uninstall: jaxlib\n",
      "    Found existing installation: jaxlib 0.7.2\n",
      "    Uninstalling jaxlib-0.7.2:\n",
      "      Successfully uninstalled jaxlib-0.7.2\n",
      "  Attempting uninstall: jax\n",
      "    Found existing installation: jax 0.7.2\n",
      "    Uninstalling jax-0.7.2:\n",
      "      Successfully uninstalled jax-0.7.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed jax-0.7.1 jaxlib-0.7.1 mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "e1934ca2f5f8453eb5831b134f88c1d4",
       "pip_warning": {
        "packages": [
         "cv2",
         "google",
         "jax",
         "jaxlib",
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install mediapipe==0.10.21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bDgezE5DRvgs"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import os, cv2, json, random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OBVy0TUVSOPA"
   },
   "outputs": [],
   "source": [
    "DATASET_ROOT = r\"D:\\Samvad_Setu_final\\datasets\\WLASL\"\n",
    "VIDEO_DIR = os.path.join(DATASET_ROOT, \"videos\")\n",
    "JSON_PATH = os.path.join(DATASET_ROOT, \"nslt_2000.json\")\n",
    "\n",
    "CACHE_DIR = os.path.join(DATASET_ROOT, \"cache_fast\")\n",
    "MODEL_DIR = r\"D:\\Samvad_Setu_final\\notebooks\\Saved_models\"\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 160\n",
    "MAX_FRAMES = 16\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS_STAGE1 = 12\n",
    "EPOCHS_STAGE2 = 20\n",
    "BASE_LR = 3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EytEjWXlT2LH",
    "outputId": "0d0ea627-7a71-44dd-b2d7-43ed79a39a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 2000\n"
     ]
    }
   ],
   "source": [
    "with open(JSON_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "video_meta = {}\n",
    "for vid, meta in data.items():\n",
    "    vid = vid.zfill(5)\n",
    "    label = meta[\"action\"][0]\n",
    "    subset = meta[\"subset\"]\n",
    "    video_meta[vid] = (label, subset)\n",
    "\n",
    "num_classes = len(set(v[0] for v in video_meta.values()))\n",
    "print(\"Classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kdi85rUeT6Rd"
   },
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "def extract_hand_skeleton(frame):\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res = hands.process(rgb)\n",
    "    if not res.multi_hand_landmarks:\n",
    "        return np.zeros((42,), dtype=np.float16)\n",
    "\n",
    "    coords = []\n",
    "    for lm in res.multi_hand_landmarks[0].landmark:\n",
    "        coords.extend([lm.x, lm.y])\n",
    "\n",
    "    return np.array(coords, dtype=np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_kd7JgvdT8Tz"
   },
   "outputs": [],
   "source": [
    "def process_video_ultrafast(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total <= 0:\n",
    "        cap.release()\n",
    "        return None, None\n",
    "\n",
    "    idxs = np.linspace(0, total-1, MAX_FRAMES, dtype=int)\n",
    "\n",
    "    frames, skeletons = [], []\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "        frames.append(frame / 255.0)\n",
    "\n",
    "        # Skeleton only every alternate frame (latency win)\n",
    "        if i % 2 == 0:\n",
    "            skeletons.append(extract_hand_skeleton(frame))\n",
    "        else:\n",
    "            skeletons.append(np.zeros((42,), dtype=np.float16))\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) != MAX_FRAMES:\n",
    "        return None, None\n",
    "\n",
    "    return (\n",
    "        np.array(frames, dtype=np.float16),\n",
    "        np.array(skeletons, dtype=np.float16)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YquQrs3vT_Id"
   },
   "outputs": [],
   "source": [
    "def ultrafast_cache(video_meta, limit=None):\n",
    "    cached = set(f.replace(\".npz\",\"\") for f in os.listdir(CACHE_DIR))\n",
    "    todo = [vid for vid in video_meta if vid not in cached]\n",
    "\n",
    "    if limit:\n",
    "        todo = todo[:limit]\n",
    "\n",
    "    print(f\"Cached: {len(cached)} | Remaining: {len(todo)}\")\n",
    "\n",
    "    for vid in tqdm(todo, desc=\"Ultra-fast caching\"):\n",
    "        vp = os.path.join(VIDEO_DIR, f\"{vid}.mp4\")\n",
    "        if not os.path.exists(vp):\n",
    "            continue\n",
    "\n",
    "        frames, skel = process_video_ultrafast(vp)\n",
    "        if frames is None:\n",
    "            continue\n",
    "\n",
    "        label, _ = video_meta[vid]\n",
    "        np.savez_compressed(\n",
    "            os.path.join(CACHE_DIR, f\"{vid}.npz\"),\n",
    "            frames=frames,\n",
    "            skeleton=skel,\n",
    "            label=label\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-H-7wffUCIs",
    "outputId": "1c561325-e185-4488-a3e0-9030ad4fed7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached: 1102 | Remaining: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultra-fast caching: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [06:24<00:00,  5.20it/s]\n"
     ]
    }
   ],
   "source": [
    "ultrafast_cache(video_meta, limit=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aeNYXK1BUErR"
   },
   "outputs": [],
   "source": [
    "def load_npz(path):\n",
    "    d = np.load(path.numpy().decode())\n",
    "    return d[\"frames\"], d[\"skeleton\"], d[\"label\"]\n",
    "\n",
    "def tf_loader(path):\n",
    "    f, s, l = tf.py_function(\n",
    "        load_npz, [path],\n",
    "        [tf.float16, tf.float16, tf.int32]\n",
    "    )\n",
    "    f.set_shape((MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3))\n",
    "    s.set_shape((MAX_FRAMES, 42))\n",
    "    l.set_shape(())\n",
    "    return (f, s), l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AmN3LJ67UXMr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 199 42\n"
     ]
    }
   ],
   "source": [
    "paths = [os.path.join(CACHE_DIR, f) for f in os.listdir(CACHE_DIR)]\n",
    "random.shuffle(paths)\n",
    "\n",
    "split1 = int(0.7 * len(paths))\n",
    "split2 = int(0.85 * len(paths))\n",
    "\n",
    "train_p = paths[:split1]\n",
    "val_p   = paths[split1:split2]\n",
    "test_p  = paths[split2:]\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(train_p)\n",
    "    .map(tf_loader, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .shuffle(1024)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(val_p)\n",
    "    .map(tf_loader, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "STEPS_PER_EPOCH = max(1, len(train_p)//BATCH_SIZE)\n",
    "VAL_STEPS = max(1, len(val_p)//BATCH_SIZE)\n",
    "\n",
    "print(\"Steps:\", STEPS_PER_EPOCH, VAL_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BpFJOXajUZgC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n",
      "\u001b[1m4334752/4334752\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "video_in = layers.Input((MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "base = tf.keras.applications.MobileNetV3Small(\n",
    "    include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "x = layers.TimeDistributed(base)(video_in)\n",
    "x = layers.GRU(128)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "out = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "\n",
    "stage1_model = models.Model(video_in, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBCW-409UcMM",
    "outputId": "ad8e243d-82d6-495c-80b0-999d8d8b7672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2219s\u001b[0m 9s/step - accuracy: 0.0038 - loss: 7.4799 - sparse_top_k_categorical_accuracy: 0.0088 - val_accuracy: 0.0060 - val_loss: 7.2689 - val_sparse_top_k_categorical_accuracy: 0.0119\n",
      "Epoch 2/30\n",
      "\u001b[1m199/199\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2812s\u001b[0m 14s/step - accuracy: 0.0025 - loss: 6.5041 - sparse_top_k_categorical_accuracy: 0.0240 - val_accuracy: 0.0000e+00 - val_loss: 7.5552 - val_sparse_top_k_categorical_accuracy: 0.0060\n",
      "Epoch 3/30\n",
      "\u001b[1m  3/199\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m17:44\u001b[0m 5s/step - accuracy: 0.0000e+00 - loss: 6.6456 - sparse_top_k_categorical_accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "\n",
    "stage1_model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(BASE_LR),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    ")\n",
    "\n",
    "stage1_model.fit(\n",
    "    train_ds.map(lambda x,y:(x[0],y)),\n",
    "    validation_data=val_ds.map(lambda x,y:(x[0],y)),\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sggxnRTdUees"
   },
   "outputs": [],
   "source": [
    "class GraphConv(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.fc = layers.Dense(units)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape will be (batch_size, num_nodes, feature_dim)\n",
    "        # The Dense layer transforms the last dimension\n",
    "        return (input_shape[0], input_shape[1], self.fc.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6JAjoSeUgjv"
   },
   "outputs": [],
   "source": [
    "skel_in = layers.Input((MAX_FRAMES, 42))\n",
    "s = layers.Reshape((MAX_FRAMES, 21, 2))(skel_in)\n",
    "s = layers.TimeDistributed(GraphConv(64))(s)\n",
    "s = layers.TimeDistributed(layers.GlobalAveragePooling1D())(s)\n",
    "s = layers.GRU(64)(s)\n",
    "\n",
    "fusion = layers.Concatenate()([stage1_model.output, s])\n",
    "fusion = layers.Dense(256, activation=\"relu\")(fusion)\n",
    "out = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(fusion)\n",
    "\n",
    "final_model = models.Model(\n",
    "    inputs=[video_in, skel_in],\n",
    "    outputs=out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ju2rkQhiUlXm",
    "outputId": "7f49a03a-c143-4c3c-9f85-bd1ee460b870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - accuracy: 0.0000e+00 - loss: 7.5813 - top5: 0.0048\n",
      "Epoch 1: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_001.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 734ms/step - accuracy: 0.0000e+00 - loss: 7.5809 - top5: 0.0048 - val_accuracy: 0.0000e+00 - val_loss: 7.1641 - val_top5: 0.0069 - learning_rate: 3.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530ms/step - accuracy: 0.0019 - loss: 6.5346 - top5: 0.0188\n",
      "Epoch 2: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_002.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 659ms/step - accuracy: 0.0019 - loss: 6.5342 - top5: 0.0188 - val_accuracy: 0.0000e+00 - val_loss: 7.2806 - val_top5: 0.0139 - learning_rate: 3.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521ms/step - accuracy: 0.0133 - loss: 6.2155 - top5: 0.0262\n",
      "Epoch 3: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_003.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 830ms/step - accuracy: 0.0133 - loss: 6.2152 - top5: 0.0262 - val_accuracy: 0.0000e+00 - val_loss: 7.4599 - val_top5: 0.0139 - learning_rate: 3.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - accuracy: 0.0079 - loss: 6.0834 - top5: 0.0317\n",
      "Epoch 4: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_004.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 571ms/step - accuracy: 0.0078 - loss: 6.0835 - top5: 0.0317 - val_accuracy: 0.0000e+00 - val_loss: 7.7078 - val_top5: 0.0139 - learning_rate: 3.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523ms/step - accuracy: 0.0067 - loss: 6.0118 - top5: 0.0316\n",
      "Epoch 5: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_005.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 718ms/step - accuracy: 0.0067 - loss: 6.0119 - top5: 0.0316 - val_accuracy: 0.0000e+00 - val_loss: 7.7901 - val_top5: 0.0139 - learning_rate: 9.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450ms/step - accuracy: 0.0078 - loss: 5.9906 - top5: 0.0366\n",
      "Epoch 6: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_006.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 644ms/step - accuracy: 0.0078 - loss: 5.9907 - top5: 0.0366 - val_accuracy: 0.0000e+00 - val_loss: 7.7381 - val_top5: 0.0139 - learning_rate: 9.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.0058 - loss: 6.0067 - top5: 0.0339\n",
      "Epoch 7: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_007.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 641ms/step - accuracy: 0.0058 - loss: 6.0068 - top5: 0.0339 - val_accuracy: 0.0000e+00 - val_loss: 7.7821 - val_top5: 0.0139 - learning_rate: 9.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - accuracy: 0.0108 - loss: 5.9610 - top5: 0.0507\n",
      "Epoch 8: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_008.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 755ms/step - accuracy: 0.0108 - loss: 5.9612 - top5: 0.0507 - val_accuracy: 0.0000e+00 - val_loss: 7.8374 - val_top5: 0.0139 - learning_rate: 2.7000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543ms/step - accuracy: 0.0080 - loss: 6.0513 - top5: 0.0328\n",
      "Epoch 9: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_009.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 722ms/step - accuracy: 0.0080 - loss: 6.0510 - top5: 0.0328 - val_accuracy: 0.0000e+00 - val_loss: 7.8438 - val_top5: 0.0139 - learning_rate: 2.7000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step - accuracy: 0.0079 - loss: 5.9813 - top5: 0.0366\n",
      "Epoch 10: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_010.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 647ms/step - accuracy: 0.0079 - loss: 5.9814 - top5: 0.0366 - val_accuracy: 0.0000e+00 - val_loss: 7.8955 - val_top5: 0.0139 - learning_rate: 2.7000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.0029 - loss: 6.0101 - top5: 0.0205\n",
      "Epoch 11: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_011.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 646ms/step - accuracy: 0.0029 - loss: 6.0101 - top5: 0.0205 - val_accuracy: 0.0000e+00 - val_loss: 7.8925 - val_top5: 0.0139 - learning_rate: 8.1000e-06\n",
      "Epoch 12/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 445ms/step - accuracy: 0.0179 - loss: 5.9332 - top5: 0.0632\n",
      "Epoch 12: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_012.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 638ms/step - accuracy: 0.0178 - loss: 5.9334 - top5: 0.0631 - val_accuracy: 0.0000e+00 - val_loss: 7.9022 - val_top5: 0.0139 - learning_rate: 8.1000e-06\n",
      "Epoch 13/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462ms/step - accuracy: 0.0042 - loss: 5.9714 - top5: 0.0361\n",
      "Epoch 13: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_013.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 767ms/step - accuracy: 0.0043 - loss: 5.9714 - top5: 0.0361 - val_accuracy: 0.0000e+00 - val_loss: 7.9155 - val_top5: 0.0139 - learning_rate: 8.1000e-06\n",
      "Epoch 14/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444ms/step - accuracy: 0.0034 - loss: 5.9950 - top5: 0.0237\n",
      "Epoch 14: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_014.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 829ms/step - accuracy: 0.0034 - loss: 5.9950 - top5: 0.0237 - val_accuracy: 0.0000e+00 - val_loss: 7.9127 - val_top5: 0.0139 - learning_rate: 2.4300e-06\n",
      "Epoch 15/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - accuracy: 0.0112 - loss: 5.9552 - top5: 0.0517\n",
      "Epoch 15: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_015.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 639ms/step - accuracy: 0.0112 - loss: 5.9554 - top5: 0.0516 - val_accuracy: 0.0000e+00 - val_loss: 7.9134 - val_top5: 0.0139 - learning_rate: 2.4300e-06\n",
      "Epoch 16/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.0059 - loss: 5.9445 - top5: 0.0446\n",
      "Epoch 16: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_016.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 759ms/step - accuracy: 0.0059 - loss: 5.9446 - top5: 0.0445 - val_accuracy: 0.0000e+00 - val_loss: 7.9165 - val_top5: 0.0139 - learning_rate: 2.4300e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561ms/step - accuracy: 0.0071 - loss: 5.9652 - top5: 0.0205\n",
      "Epoch 17: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_017.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 723ms/step - accuracy: 0.0071 - loss: 5.9653 - top5: 0.0206 - val_accuracy: 0.0000e+00 - val_loss: 7.9179 - val_top5: 0.0139 - learning_rate: 7.2900e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - accuracy: 0.0017 - loss: 6.0470 - top5: 0.0208\n",
      "Epoch 18: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_018.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 644ms/step - accuracy: 0.0017 - loss: 6.0467 - top5: 0.0209 - val_accuracy: 0.0000e+00 - val_loss: 7.9182 - val_top5: 0.0139 - learning_rate: 7.2900e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - accuracy: 0.0057 - loss: 5.9995 - top5: 0.0280\n",
      "Epoch 19: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_019.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 649ms/step - accuracy: 0.0057 - loss: 5.9994 - top5: 0.0280 - val_accuracy: 0.0000e+00 - val_loss: 7.9184 - val_top5: 0.0139 - learning_rate: 7.2900e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - accuracy: 0.0136 - loss: 5.9847 - top5: 0.0396\n",
      "Epoch 20: saving model to /content/drive/MyDrive/SAMVAD_SETU/Saved-models/epoch_020.keras\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 639ms/step - accuracy: 0.0136 - loss: 5.9847 - top5: 0.0396 - val_accuracy: 0.0000e+00 - val_loss: 7.9186 - val_top5: 0.0139 - learning_rate: 2.1870e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x78183897eb40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(BASE_LR),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"top5\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(MODEL_DIR, \"epoch_{epoch:03d}.keras\"),\n",
    "        save_best_only=False,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.3)\n",
    "]\n",
    "\n",
    "final_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    epochs=EPOCHS_STAGE2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y23hyp4MxgQx"
   },
   "outputs": [],
   "source": [
    "final_model.save(\"/content/drive/MyDrive/SAMVAD_SETU/Saved-models/wlasl/wlasl-final.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehJDTc-7UoZ7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "import base64, cv2, numpy as np\n",
    "\n",
    "def init_webcam():\n",
    "    display(Javascript(\"\"\"\n",
    "    async function init() {\n",
    "      const video = document.createElement('video');\n",
    "      video.setAttribute('autoplay', '');\n",
    "      video.setAttribute('playsinline', '');\n",
    "      document.body.appendChild(video);\n",
    "\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "      video.srcObject = stream;\n",
    "\n",
    "      // wait until video is ready\n",
    "      await new Promise(resolve => video.onloadedmetadata = resolve);\n",
    "\n",
    "      window._video = video;\n",
    "    }\n",
    "    init();\n",
    "    \"\"\"))\n",
    "\n",
    "def capture_frame():\n",
    "    data = eval_js(\"\"\"\n",
    "    (() => {\n",
    "      if (!window._video) return null;\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = _video.videoWidth;\n",
    "      canvas.height = _video.videoHeight;\n",
    "      const ctx = canvas.getContext('2d');\n",
    "      ctx.drawImage(_video, 0, 0);\n",
    "      return canvas.toDataURL('image/jpeg');\n",
    "    })()\n",
    "    \"\"\")\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    img_bytes = base64.b64decode(data.split(',')[1])\n",
    "    return cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "LXseXgL4OhA8",
    "outputId": "a7c009b9-db40-4327-87a6-bdca60bf9c13"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function init() {\n",
       "      const video = document.createElement('video');\n",
       "      video.setAttribute('autoplay', '');\n",
       "      video.setAttribute('playsinline', '');\n",
       "      document.body.appendChild(video);\n",
       "\n",
       "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
       "      video.srcObject = stream;\n",
       "\n",
       "      // wait until video is ready\n",
       "      await new Promise(resolve => video.onloadedmetadata = resolve);\n",
       "\n",
       "      window._video = video;\n",
       "    }\n",
       "    init();\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam initializing... wait 3â€“5 seconds, then run next cell\n"
     ]
    }
   ],
   "source": [
    "init_webcam()\n",
    "print(\"Webcam initializing... wait 3â€“5 seconds, then run next cell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "AoU_ZMS2RC78",
    "outputId": "5de2962c-0153-46a8-993d-dd77a45fb816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual testing started (Ctrl+C to stop)\n",
      "Stopped manual testing\n"
     ]
    }
   ],
   "source": [
    "buf_f, buf_s = [], []\n",
    "\n",
    "print(\"Manual testing started (Ctrl+C to stop)\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frame = capture_frame()\n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        fr = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "        buf_f.append(fr / 255.0)\n",
    "        buf_s.append(extract_hand_skeleton(fr))\n",
    "\n",
    "        # Draw ROI\n",
    "        h, w, _ = frame.shape\n",
    "        size = 300\n",
    "        x1, y1 = w//2 - size//2, h//2 - size//2\n",
    "        x2, y2 = x1 + size, y1 + size\n",
    "        cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "        if len(buf_f) == MAX_FRAMES:\n",
    "            video = np.expand_dims(buf_f, 0)\n",
    "            skel  = np.expand_dims(buf_s, 0)\n",
    "\n",
    "            if np.sum(video) < 1e-3:\n",
    "                pred = final_model.predict(\n",
    "                    [np.zeros_like(video), skel],\n",
    "                    verbose=0\n",
    "                )\n",
    "            else:\n",
    "                pred = final_model.predict(\n",
    "                    [video, skel],\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "            cls = int(np.argmax(pred))\n",
    "            conf = float(np.max(pred)) * 100\n",
    "\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"Pred: {cls} ({conf:.1f}%)\",\n",
    "                (20,40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (0,255,0),\n",
    "                2\n",
    "            )\n",
    "\n",
    "            buf_f.pop(0)\n",
    "            buf_s.pop(0)\n",
    "\n",
    "        display(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped manual testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "ATf5GVWrRHOf",
    "outputId": "b95e285b-5a53-4d01-f5ef-7c1d3db4fc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Manual testing started (Ctrl+C to stop)\n",
      "ğŸ›‘ Manual testing stopped\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "MAX_FRAMES = 12        # ğŸ”¥ faster feedback\n",
    "CONF_THRESH = 0.30    # ignore junk predictions\n",
    "\n",
    "buf_f, buf_s = [], []\n",
    "\n",
    "print(\"ğŸŸ¢ Manual testing started (Ctrl+C to stop)\")\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frame = capture_frame()\n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # ---------- Preprocess ----------\n",
    "        fr = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "        fr = fr / 255.0\n",
    "        sk = extract_hand_skeleton(fr)\n",
    "\n",
    "        buf_f.append(fr)\n",
    "        buf_s.append(sk)\n",
    "\n",
    "        # Keep rolling buffer\n",
    "        if len(buf_f) > MAX_FRAMES:\n",
    "            buf_f.pop(0)\n",
    "            buf_s.pop(0)\n",
    "\n",
    "        # ---------- UI ----------\n",
    "        h, w, _ = frame.shape\n",
    "        cv2.rectangle(\n",
    "            frame,\n",
    "            (w//2-150, h//2-150),\n",
    "            (w//2+150, h//2+150),\n",
    "            (0,255,0), 2\n",
    "        )\n",
    "\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"Buffer: {len(buf_f)}/{MAX_FRAMES}\",\n",
    "            (20,30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.9, (255,255,0), 2\n",
    "        )\n",
    "\n",
    "        # ---------- Predict ----------\n",
    "        if len(buf_f) == MAX_FRAMES:\n",
    "            video = np.expand_dims(np.array(buf_f), 0)\n",
    "            skel  = np.expand_dims(np.array(buf_s), 0)\n",
    "\n",
    "            # Fallback logic\n",
    "            if np.sum(video) < 1e-3 or np.sum(skel) < 1e-3:\n",
    "                pred = final_model.predict(\n",
    "                    [np.zeros_like(video), skel],\n",
    "                    verbose=0\n",
    "                )\n",
    "                fallback = \"Skeleton\"\n",
    "            else:\n",
    "                pred = final_model.predict(\n",
    "                    [video, skel],\n",
    "                    verbose=0\n",
    "                )\n",
    "                fallback = \"Video+Skeleton\"\n",
    "\n",
    "            cls = int(np.argmax(pred))\n",
    "            conf = float(np.max(pred))\n",
    "\n",
    "            if conf > CONF_THRESH:\n",
    "                label = ID2GLOSS.get(cls, f\"Class-{cls}\")\n",
    "                txt = f\"{label}  {conf*100:.1f}%  [{fallback}]\"\n",
    "            else:\n",
    "                txt = \"Low confidence...\"\n",
    "\n",
    "            cv2.putText(\n",
    "                frame, txt,\n",
    "                (20,70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.9, (0,255,0), 2\n",
    "            )\n",
    "\n",
    "        # ---------- Display ----------\n",
    "        clear_output(wait=True)\n",
    "        display(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"ğŸ›‘ Manual testing stopped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVVtmurKUFrF",
    "outputId": "b7abf168-30e5-4f78-cf35-845f4b5d4e24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor shape=(None, 16, 160, 160, 3), dtype=float32, sparse=False, ragged=False, name=keras_tensor>,\n",
       " <KerasTensor shape=(None, 16, 42), dtype=float32, sparse=False, ragged=False, name=keras_tensor_357>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "J68jKwRgUX-6",
    "outputId": "f1b97196-afa0-4c30-b5d4-46e92e230f63"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1807183401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_hand_skeleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fr' is not defined"
     ]
    }
   ],
   "source": [
    "sk = extract_hand_skeleton(fr)\n",
    "print(np.sum(sk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "Zb0o6zrYUtpx",
    "outputId": "bc82b45a-82ec-40c6-f8f9-8a0e3e7b9240"
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-559877851.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "frame = capture_frame()\n",
    "frame = cv2.flip(frame, 1)\n",
    "\n",
    "fr = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "fr = fr / 255.0\n",
    "\n",
    "sk = extract_hand_skeleton(fr)\n",
    "\n",
    "print(\"Skeleton sum:\", np.sum(sk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bau45UoeUxfb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
